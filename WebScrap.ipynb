{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe99a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Legen\\AppData\\Local\\Temp/ipykernel_15468/2547559511.py:91: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  browser = webdriver.Chrome(r\"C:\\Users\\Legen\\Downloads\\Compressed\\chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# importing the libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support import select\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import random\n",
    "import os\n",
    "\n",
    "\n",
    "def get_soup(url, header):\n",
    "\n",
    "    html_content = requests.get(url,headers = header).text\n",
    "    # Parse the html content\n",
    "    soup = BeautifulSoup(html_content, \"html\")\n",
    "    return soup\n",
    "\n",
    "def download_webpage(url, header):\n",
    "\n",
    "    html_content = requests.get(url,headers = header).content\n",
    "    soup = BeautifulSoup(html_content)\n",
    "    with open(\"test.txt\", \"w\") as f:\n",
    "        f.write(soup.prettify())\n",
    "\n",
    "def readFromFile(filename):\n",
    "    with open(filename) as f:\n",
    "        soup = BeautifulSoup(f,'lxml')\n",
    "        return soup\n",
    "\n",
    "\n",
    "def get_headers(table):\n",
    "    t1 = table.find(\"thead\")\n",
    "    return [th.text.strip() for th in t1.find(\"tr\").find_all(\"th\")]\n",
    " \n",
    "\n",
    "def get_all_rows(tb):\n",
    "    table = tb.find('tbody')\n",
    "    return [[td.text.strip() for td in tr.find_all(\"td\")] for tr in table.find_all(\"tr\")]\n",
    "\n",
    "    from selenium.webdriver.common.by import By\n",
    "\n",
    "def click_and_download(number,delay, table_list, browser):\n",
    "\n",
    "    if number > 5:\n",
    "        i = 4\n",
    "    else:\n",
    "        i = number\n",
    "\n",
    "    xpath1 = f'/html/body/div[2]/div/section[2]/div[3]/div/div/div/div[2]/div/div[1]/div[2]/div/div[8]/div/div/div[5]/span/a[{i}]'\n",
    "    button = browser.find_element(By.XPATH,xpath1)\n",
    "    \n",
    "    button.click()\n",
    "\n",
    "    WebDriverWait(browser, delay).until(\n",
    "        EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "    )\n",
    "    time.sleep(random.randint(5,10))\n",
    "    html = browser.page_source\n",
    "    table_list.append(get_table(html))\n",
    "\n",
    "def find_table(soup):\n",
    "    table = soup.find('table', { 'id': 'myTableCPriceHistory'})\n",
    "    return table\n",
    "\n",
    "def get_table(html,number=1):\n",
    "    if html:\n",
    "        soup = BeautifulSoup(html, 'lxml')        \n",
    "        # with open(f\"adbl-{number}.txt\", \"w\") as f:\n",
    "        #     f.write(soup.prettify())\n",
    "        return find_table(soup)\n",
    "\n",
    "  \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    company_symbol = 'ahpc'\n",
    "\n",
    "\n",
    "    html = None\n",
    "    url = f'https://www.sharesansar.com/company/{company_symbol}'\n",
    "    selector = '#myTableCPriceHistory > tbody:nth-child(2) > tr:nth-child(1)'\n",
    "    delay = 20  # seconds\n",
    "    table_list = []\n",
    "\n",
    "    browser = webdriver.Chrome(r\"C:\\Users\\Legen\\Downloads\\Compressed\\chromedriver.exe\")\n",
    "\n",
    "    browser.maximize_window()\n",
    "    browser.get(url)\n",
    "\n",
    "    try:\n",
    "        # wait for button to be enabled\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.element_to_be_clickable((By.XPATH, '//*[@id=\"btn_cpricehistory\"]'))\n",
    "        )\n",
    "        button = browser.find_element(By.XPATH,'//*[@id=\"btn_cpricehistory\"]')\n",
    "        button.click()\n",
    "\n",
    "        # wait for data to be loaded\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "        )\n",
    "\n",
    "        select = select.Select(browser.find_element(By.CSS_SELECTOR,'#myTableCPriceHistory_length > label:nth-child(1) > select:nth-child(1)'))\n",
    "        select.select_by_visible_text(\"50\")\n",
    "\n",
    "        WebDriverWait(browser, delay).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, selector))\n",
    "        )\n",
    "        time.sleep(5)\n",
    "        html = browser.page_source\n",
    "        table_list.append(get_table(html))\n",
    "    except TimeoutException:\n",
    "        print('Loading took too much time!')\n",
    "    \n",
    "\n",
    "    for i in range(2,56): # Instead of 5, Put a number of the page in table till which you want to extract table\n",
    "        try:\n",
    "            click_and_download(i,delay, table_list, browser)\n",
    "        except TimeoutException:\n",
    "            print('Loading took too much time!')\n",
    "            break\n",
    "\n",
    "\n",
    "    browser.quit()\n",
    "\n",
    "    headers = get_headers(table_list[0])\n",
    "\n",
    "\n",
    "    with open(f'{company_symbol}.csv','w') as c:\n",
    "        csv_writer = csv.writer(c, delimiter=',', quotechar='\"', \n",
    "        quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        csv_writer.writerow(headers)\n",
    "\n",
    "        for table in table_list:\n",
    "\n",
    "            rowlist = get_all_rows(table)\n",
    "            for row in rowlist:\n",
    "                csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b42a14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lxmlNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading lxml-4.8.0-cp39-cp39-win_amd64.whl (3.6 MB)\n",
      "Installing collected packages: lxml\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Legen\\Desktop\\Major Project\\env\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed lxml-4.8.0\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82422eda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
